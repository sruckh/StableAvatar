# CLAUDE.md
<!-- Generated by Claude Conductor v1.1.2 -->

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Critical Context (Read First)
- **Tech Stack**: Python 3.8+, PyTorch 2.6.0, Diffusers 0.30.1, Transformers 4.51.3, CUDA 12.4
- **Main File**: inference.py:570 lines (inference), train_1B_square.py:1500+ lines (training)
- **Core Mechanic**: Audio-driven infinite-length avatar video generation using diffusion transformers
- **Key Integration**: HuggingFace Hub, Wav2Vec2, CLIP, xFuser, DeepSpeed, Accelerate
- **Platform Support**: Linux/CUDA required (18GB+ VRAM), multi-GPU distributed training
- **DO NOT**: Commit model checkpoints, modify code without .gitignore, train without sufficient VRAM

## Session Startup Checklist
**IMPORTANT**: At the start of each session, check these items:
1. **Check TASKS.md** - Look for any IN_PROGRESS or BLOCKED tasks from previous sessions
2. **Review recent JOURNAL.md entries** - Scan last 2-3 entries for context
3. **If resuming work**: Load the current task context from TASKS.md before proceeding

## Table of Contents
1. [Architecture](ARCHITECTURE.md) - Tech stack, folder structure, infrastructure
2. [Design Tokens](DESIGN.md) - Colors, typography, visual system
3. [UI/UX Patterns](UIUX.md) - Components, interactions, accessibility
4. [Runtime Config](CONFIG.md) - Environment variables, feature flags
5. [Data Model](DATA_MODEL.md) - Database schema, entities, relationships
6. [API Contracts](API.md) - Endpoints, request/response formats, auth
7. [Build & Release](BUILD.md) - Build process, deployment, CI/CD
8. [Testing Guide](TEST.md) - Test strategies, E2E scenarios, coverage
9. [Operational Playbooks](PLAYBOOKS/DEPLOY.md) - Deployment, rollback, monitoring
10. [Contributing](CONTRIBUTING.md) - Code style, PR process, conventions
11. [Error Ledger](ERRORS.md) - Critical P0/P1 error tracking
12. [Task Management](TASKS.md) - Active tasks, phase tracking, context preservation

## Quick Reference
**Main Inference**: `inference.py:405-570` - Main inference pipeline setup  
**Argument Parser**: `inference.py:237-402` - Command line argument definitions  
**Training Main**: `train_1B_square.py:main()` - Training loop implementation  
**WAN Transformer**: `wan/models/wan_fantasy_transformer3d_1B.py` - Core 1.3B model  
**Audio Projector**: `wan/models/vocal_projector_fantasy_1B.py` - Audio processing  
**VAE Encoder**: `wan/models/wan_vae.py` - Video encoding/decoding  
**Inference Pipeline**: `wan/pipeline/wan_inference_long_pipeline.py` - Long video pipeline  
**Audio Utils**: `audio_extractor.py` - Audio extraction from video  
**Vocal Separator**: `vocal_seperator.py` - Background music separation  
**Lip Masking**: `lip_mask_extractor.py` - Lip mask generation for training  
**Config Management**: `wan/configs/` - Model configuration files  
**Dataset Loading**: `wan/dataset/talking_video_dataset_fantasy.py` - Training data loader

## Current State
- [x] StableAvatar-1.3B basic model inference
- [x] Training code for 1.3B model (square and mixed resolution)
- [x] Audio/video preprocessing utilities
- [x] Multi-GPU parallel inference support
- [ ] LoRA training/finetuning code (ETA: Aug 2025)
- [ ] Audio Native Guidance inference
- [ ] StableAvatar-pro model release
- [ ] Full evaluation dataset release

## Development Workflow
1. **Environment Setup**: Install PyTorch 2.6.0 + CUDA 12.4, pip install -r requirements.txt
2. **Download Models**: Use huggingface-cli to download StableAvatar and Wan2.1 weights to checkpoints/
3. **Test Inference**: Run inference.sh with sample data from examples/ directory
4. **Prepare Training Data**: Extract audio/video, generate face/lip masks, organize dataset structure
5. **Train/Finetune**: Use train_1B_square.sh or train_1B_rec_vec.sh based on resolution needs
6. **Validate Results**: Check generated videos for quality, lip sync, and identity preservation

## Task Templates
### 1. Basic Inference Test
1. Check GPU memory: `nvidia-smi` (need 18GB+ free)
2. Verify checkpoint path in `inference.sh:7` points to correct model
3. Run test: `bash inference.sh` 
4. Validate output video created in output_infer/ directory

### 2. Add New Model Variant
1. Create new config in `wan/configs/` following existing pattern
2. Add model class in `wan/models/` inheriting from base transformer
3. Update `inference.py:473-485` to handle new model loading
4. Test with minimal inference to verify loading

### 3. Audio Processing Enhancement
1. Modify audio projection in `wan/models/vocal_projector_fantasy_1B.py:392-398`
2. Update config in relevant YAML file for new parameters
3. Test with `python audio_extractor.py` on sample data
4. Verify audio features maintain expected dimensions

### 4. Memory Optimization
1. Analyze memory usage patterns in `inference.py:497-510`
2. Implement new GPU memory mode in conditional blocks
3. Test with `--GPU_memory_mode` parameter variations
4. Document new mode in inference.sh comments

## Anti-Patterns (Avoid These)
❌ **Don't commit model checkpoints** - Files are 2GB+ and belong in checkpoints/ with .gitignore  
❌ **Don't modify code without adequate VRAM** - Training needs 40GB+, inference needs 18GB+ 
❌ **Don't skip vocal separation** - Background music degrades lip sync quality significantly
❌ **Don't use relative paths for models** - Always use absolute paths in shell scripts for reliability
❌ **Don't ignore GPU memory modes** - Use sequential_cpu_offload for low VRAM, model_cpu_offload for medium
❌ **Don't mix resolution training data** - Keep 512x512, 480x832, 832x480 datasets separate

## Journal Update Requirements
**IMPORTANT**: Update JOURNAL.md regularly throughout our work sessions:
- After completing any significant feature or fix
- When encountering and resolving errors
- At the end of each work session
- When making architectural decisions
- Format: What/Why/How/Issues/Result structure

## Task Management Integration
**How TASKS.md and JOURNAL.md work together**:
1. **Active Work**: TASKS.md tracks current/incomplete tasks with full context
2. **Completed Work**: When tasks complete, they generate JOURNAL.md entries with `|TASK:ID|` tags
3. **History**: JOURNAL.md preserves complete task history even if Claude Code is reinstalled
4. **Context Recovery**: Search JOURNAL.md for `|TASK:` to see all completed tasks over time
5. **Clean Handoffs**: TASKS.md always shows what needs to be resumed or completed

## Version History
- **v1.0.0** - Initial release
- **v1.1.0** - Feature added (see JOURNAL.md YYYY-MM-DD)  
[Link major versions to journal entries]